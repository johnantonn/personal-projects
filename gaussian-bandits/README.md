# gaussian-bandits
Experimentation with Gaussian multi-armed bandits and action selection functions.

![image](https://user-images.githubusercontent.com/8168416/157848879-8d64be9f-73b8-48aa-b7c3-d5236691cb1c.png)

## Description
The notebook contains a simple definition of a bandit and arm classes, as well as different functions for selecting the next action (arm to pull). The implementation is based on the referenced book by S. Sutton and A. Barto (see below).

*Note*: The functionality of the notebook can easily be extended by defining different types of arms, reward distributions and action selection functions and compare them.

## References

[1] [Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA.](https://dl.acm.org/doi/10.5555/3312046)

[2] [Lattimore, T., & Szepesv√°ri, C. (2020). Bandit Algorithms. Cambridge: Cambridge University Press. doi:10.1017/9781108571401
](https://tor-lattimore.com/downloads/book/book.pdf)
